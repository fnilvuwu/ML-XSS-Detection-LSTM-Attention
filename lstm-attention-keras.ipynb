{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['glove-global-vectors-for-word-representation', 'jigsaw-unintended-bias-in-toxicity-classification']\n",
      "['glove.6B.200d.txt', 'glove.6B.100d.txt', 'glove.6B.50d.txt']\n",
      "['sample_submission.csv', 'test.csv', 'train.csv']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import keras.backend as K\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.engine.topology import Layer\n",
    "from keras import initializers, regularizers, constraints\n",
    "from keras.layers import *\n",
    "from keras.models import Model\n",
    "\n",
    "import os\n",
    "print(os.listdir(\"../input\"))\n",
    "print(os.listdir(\"../input/glove-global-vectors-for-word-representation\"))\n",
    "print(os.listdir(\"../input/jigsaw-unintended-bias-in-toxicity-classification\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "source": [
    "# Data Loading\n",
    "\n",
    "we load the dataset and apply some transformations to use it in a deep learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_uuid": "ed0303324540aeb4e1068268abf27fdbef0ad5f9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Train shape: (1804874, 45)\n",
      "Test shape: (97320, 2)\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading data...\")\n",
    "df_train = pd.read_csv(\"../input/jigsaw-unintended-bias-in-toxicity-classification/train.csv\")\n",
    "print(\"Train shape:\", df_train.shape)\n",
    "df_test = pd.read_csv(\"../input/jigsaw-unintended-bias-in-toxicity-classification/test.csv\",encoding=\"latin-1\")\n",
    "print(\"Test shape:\", df_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.rename(columns=({\"comment_text\":\"Reviews\"}))\n",
    "df_train = df_train.rename(columns=({\"target\":\"Label\"}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df_test.rename(columns=({\"comment_text\":\"Reviews\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train[:10000]\n",
    "df_test = df_test[:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Label</th>\n",
       "      <th>Reviews</th>\n",
       "      <th>severe_toxicity</th>\n",
       "      <th>obscene</th>\n",
       "      <th>identity_attack</th>\n",
       "      <th>insult</th>\n",
       "      <th>threat</th>\n",
       "      <th>asian</th>\n",
       "      <th>atheist</th>\n",
       "      <th>bisexual</th>\n",
       "      <th>black</th>\n",
       "      <th>buddhist</th>\n",
       "      <th>christian</th>\n",
       "      <th>female</th>\n",
       "      <th>heterosexual</th>\n",
       "      <th>hindu</th>\n",
       "      <th>homosexual_gay_or_lesbian</th>\n",
       "      <th>intellectual_or_learning_disability</th>\n",
       "      <th>jewish</th>\n",
       "      <th>latino</th>\n",
       "      <th>male</th>\n",
       "      <th>muslim</th>\n",
       "      <th>other_disability</th>\n",
       "      <th>other_gender</th>\n",
       "      <th>other_race_or_ethnicity</th>\n",
       "      <th>other_religion</th>\n",
       "      <th>other_sexual_orientation</th>\n",
       "      <th>physical_disability</th>\n",
       "      <th>psychiatric_or_mental_illness</th>\n",
       "      <th>transgender</th>\n",
       "      <th>white</th>\n",
       "      <th>created_date</th>\n",
       "      <th>publication_id</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>article_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>funny</th>\n",
       "      <th>wow</th>\n",
       "      <th>sad</th>\n",
       "      <th>likes</th>\n",
       "      <th>disagree</th>\n",
       "      <th>sexual_explicit</th>\n",
       "      <th>identity_annotator_count</th>\n",
       "      <th>toxicity_annotator_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>59848</td>\n",
       "      <td>0.0</td>\n",
       "      <td>This is so cool. It's like, 'would you want yo...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-09-29 10:50:41.987077+00</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2006</td>\n",
       "      <td>rejected</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>59849</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Thank you!! This would make my life a lot less...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-09-29 10:50:42.870083+00</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2006</td>\n",
       "      <td>rejected</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id            ...             toxicity_annotator_count\n",
       "0  59848            ...                                    4\n",
       "1  59849            ...                                    4\n",
       "\n",
       "[2 rows x 45 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'Label', 'Reviews', 'severe_toxicity', 'obscene',\n",
       "       'identity_attack', 'insult', 'threat', 'asian', 'atheist', 'bisexual',\n",
       "       'black', 'buddhist', 'christian', 'female', 'heterosexual', 'hindu',\n",
       "       'homosexual_gay_or_lesbian', 'intellectual_or_learning_disability',\n",
       "       'jewish', 'latino', 'male', 'muslim', 'other_disability',\n",
       "       'other_gender', 'other_race_or_ethnicity', 'other_religion',\n",
       "       'other_sexual_orientation', 'physical_disability',\n",
       "       'psychiatric_or_mental_illness', 'transgender', 'white', 'created_date',\n",
       "       'publication_id', 'parent_id', 'article_id', 'rating', 'funny', 'wow',\n",
       "       'sad', 'likes', 'disagree', 'sexual_explicit',\n",
       "       'identity_annotator_count', 'toxicity_annotator_count'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Reviews</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7000000</td>\n",
       "      <td>Jeff Sessions is another one of Trump's Orwell...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                            Reviews\n",
       "0  7000000  Jeff Sessions is another one of Trump's Orwell..."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "06fb1f160cb506879ee2cbb2fde5b8551a533e87"
   },
   "source": [
    "In addition to the datasets, we load a pretrained word embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_uuid": "f9f6690b12a951d72e741e221c627f835d3d9a39"
   },
   "outputs": [],
   "source": [
    "EMBEDDING_FILE =  '../input/glove-global-vectors-for-word-representation/glove.6B.200d.txt'\n",
    "\n",
    "def load_embeddings(filename):\n",
    "    embeddings = {}\n",
    "    with open(filename) as f:\n",
    "        for line in f:\n",
    "            values = line.rstrip().split(' ')\n",
    "            word = values[0]\n",
    "            vector = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings[word] = vector\n",
    "    return embeddings\n",
    "\n",
    "embeddings = load_embeddings(EMBEDDING_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "5defa17c3f32e993e862b7ddaa4f190ab4dc9cac"
   },
   "source": [
    "# Preprocessings\n",
    "\n",
    "After loading the datasets, we will preprocess the datasets. In this time, we will apply following techniques:\n",
    "\n",
    "* Nigation handling\n",
    "* Replacing numbers\n",
    "* Tokenization\n",
    "* Zero padding\n",
    "\n",
    "Negation handling is the process of converting negation abbreviation to a canonical format. For example, \"aren't\" is converted to \"are not\". It is helpful for sentiment analysis.\n",
    "\n",
    "Replacing numbers is the process of converting numbers to a specific character. For example, \"1924\" and \"123\" are converted to \"0\". It is helpful for reducing the vocabulary size.\n",
    "\n",
    "Tokenization is the process of taking a text or set of texts and breaking it up into its individual words. In this step, we will tokenize text with the help of splitting text by space or punctuation marks.\n",
    "\n",
    "Zero padding is the process of pad \"0\" to the dataset for the purpose of ensuring that all sentences has the same length."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "5c16fa877155eb7a95a6640a1ad6f4e3f8a126c8"
   },
   "source": [
    "## Negation handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_uuid": "c663ce4f9bdc2f497cf43ae21b91acc9cf6e854c"
   },
   "outputs": [],
   "source": [
    "df_train.Reviews = df_train.Reviews.str.replace(\"n't\", 'not')\n",
    "df_test.Reviews = df_test.Reviews.str.replace(\"n't\", 'not')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "lb_make = LabelEncoder()\n",
    "\n",
    "\n",
    "df_train[\"Label\"] = lb_make.fit_transform(df_train[\"Label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.000000    7500\n",
       "0.166667     653\n",
       "0.200000     543\n",
       "0.300000     286\n",
       "0.400000     260\n",
       "0.500000     151\n",
       "0.600000     111\n",
       "0.100000      70\n",
       "0.700000      54\n",
       "0.800000      30\n",
       "0.142857      23\n",
       "1.000000      18\n",
       "0.111111      15\n",
       "0.833333      11\n",
       "0.900000       9\n",
       "0.687500       4\n",
       "0.587500       4\n",
       "0.181818       3\n",
       "0.728571       3\n",
       "0.825000       3\n",
       "0.545455       3\n",
       "0.272727       3\n",
       "0.653333       3\n",
       "0.750000       2\n",
       "0.609375       2\n",
       "0.914286       2\n",
       "0.893939       2\n",
       "0.375000       2\n",
       "0.651515       2\n",
       "0.454545       2\n",
       "            ... \n",
       "0.925450       1\n",
       "0.603774       1\n",
       "0.389831       1\n",
       "0.005386       1\n",
       "0.025000       1\n",
       "0.973006       1\n",
       "0.578125       1\n",
       "0.507042       1\n",
       "0.725806       1\n",
       "0.537500       1\n",
       "0.736842       1\n",
       "0.887500       1\n",
       "0.476923       1\n",
       "0.819672       1\n",
       "0.760563       1\n",
       "0.629630       1\n",
       "0.000935       1\n",
       "0.108696       1\n",
       "0.571429       1\n",
       "0.145833       1\n",
       "0.573529       1\n",
       "0.557143       1\n",
       "0.283019       1\n",
       "0.472973       1\n",
       "0.824324       1\n",
       "0.457627       1\n",
       "0.733333       1\n",
       "0.432432       1\n",
       "0.621622       1\n",
       "0.661290       1\n",
       "Name: Label, Length: 231, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['Label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "231"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_count=len(df_train['Label'].value_counts())\n",
    "target_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                                       int64\n",
       "Label                                  float64\n",
       "Reviews                                 object\n",
       "severe_toxicity                        float64\n",
       "obscene                                float64\n",
       "identity_attack                        float64\n",
       "insult                                 float64\n",
       "threat                                 float64\n",
       "asian                                  float64\n",
       "atheist                                float64\n",
       "bisexual                               float64\n",
       "black                                  float64\n",
       "buddhist                               float64\n",
       "christian                              float64\n",
       "female                                 float64\n",
       "heterosexual                           float64\n",
       "hindu                                  float64\n",
       "homosexual_gay_or_lesbian              float64\n",
       "intellectual_or_learning_disability    float64\n",
       "jewish                                 float64\n",
       "latino                                 float64\n",
       "male                                   float64\n",
       "muslim                                 float64\n",
       "other_disability                       float64\n",
       "other_gender                           float64\n",
       "other_race_or_ethnicity                float64\n",
       "other_religion                         float64\n",
       "other_sexual_orientation               float64\n",
       "physical_disability                    float64\n",
       "psychiatric_or_mental_illness          float64\n",
       "transgender                            float64\n",
       "white                                  float64\n",
       "created_date                            object\n",
       "publication_id                           int64\n",
       "parent_id                              float64\n",
       "article_id                               int64\n",
       "rating                                  object\n",
       "funny                                    int64\n",
       "wow                                      int64\n",
       "sad                                      int64\n",
       "likes                                    int64\n",
       "disagree                                 int64\n",
       "sexual_explicit                        float64\n",
       "identity_annotator_count                 int64\n",
       "toxicity_annotator_count                 int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id          int64\n",
       "Reviews    object\n",
       "dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['Reviews'] = df_train['Reviews'].astype(str)\n",
    "df_test['Reviews'] = df_test['Reviews'].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "fee8a5b73523cd85eebd8aeea43f0c70c938f7ab"
   },
   "source": [
    "## Replacing numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_uuid": "fb4e2e2b3e346ce386ce738d8f9c755780c5ccf2"
   },
   "outputs": [],
   "source": [
    "df_train.Reviews = df_train.Reviews.apply(lambda x: re.sub(r'[0-9]+', '0', x))\n",
    "df_test.Reviews = df_test.Reviews.apply(lambda x: re.sub(r'[0-9]+', '0', x))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = df_train['Reviews'].values\n",
    "x_test  = df_test['Reviews'].values\n",
    "y_train = df_train['Label'].values\n",
    "x = np.r_[x_train, x_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "805e69b93d2932c51f87877b53a0a9c140ac7782"
   },
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_uuid": "b5d5360f1094d30dd69500b861b8a92142a01059"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary size: 83190\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(lower=True, filters='\\n\\t')\n",
    "tokenizer.fit_on_texts(x)\n",
    "x_train = tokenizer.texts_to_sequences(x_train)\n",
    "x_test  = tokenizer.texts_to_sequences(x_test)\n",
    "vocab_size = len(tokenizer.word_index) + 1  # +1 is for zero padding.\n",
    "print('vocabulary size: {}'.format(vocab_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "57e6e123b1fe89811c78dd1e503c30f4c0997c4b"
   },
   "source": [
    "## Zero padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_uuid": "aec770d09524cd17362fee54da2d4bf4dc5feb05"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maxlen: 317\n",
      "(10000, 317)\n",
      "(10000, 317)\n"
     ]
    }
   ],
   "source": [
    "maxlen = len(max((s for s in np.r_[x_train, x_test]), key=len))\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen, padding='post')\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen, padding='post')\n",
    "print('maxlen: {}'.format(maxlen))\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "_uuid": "873900aeb0026c0fce20d8df63fb839556e52d04"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOV: 54240\n"
     ]
    }
   ],
   "source": [
    "def filter_embeddings(embeddings, word_index, vocab_size, dim=300):\n",
    "    embedding_matrix = np.zeros([vocab_size, dim])\n",
    "    for word, i in word_index.items():\n",
    "        if i >= vocab_size:\n",
    "            continue\n",
    "        vector = embeddings.get(word)\n",
    "        if vector is not None:\n",
    "            embedding_matrix[i] = vector\n",
    "    return embedding_matrix\n",
    "\n",
    "embedding_size = 200\n",
    "embedding_matrix = filter_embeddings(embeddings, tokenizer.word_index,\n",
    "                                     vocab_size, embedding_size)\n",
    "print('OOV: {}'.format(len(set(tokenizer.word_index) - set(embeddings))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "10e0b9d14f0c03d97e227e3c186e6ae91575a3fb"
   },
   "source": [
    "# Building a model\n",
    "\n",
    "In this time, we will use attention based LSTM model. First of all, we should define the attention layer as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "_uuid": "e4baa9ebcbe6a13a7529c096a92b8fbe2092ed70"
   },
   "outputs": [],
   "source": [
    "class Attention(Layer):\n",
    "    \"\"\"\n",
    "    Keras Layer that implements an Attention mechanism for temporal data.\n",
    "    Supports Masking.\n",
    "    Follows the work of Raffel et al. [https://arxiv.org/abs/1512.08756]\n",
    "    # Input shape\n",
    "        3D tensor with shape: `(samples, steps, features)`.\n",
    "    # Output shape\n",
    "        2D tensor with shape: `(samples, features)`.\n",
    "    :param kwargs:\n",
    "    Just put it on top of an RNN Layer (GRU/LSTM/SimpleRNN) with return_sequences=True.\n",
    "    The dimensions are inferred based on the output shape of the RNN.\n",
    "    Example:\n",
    "        model.add(LSTM(64, return_sequences=True))\n",
    "        model.add(Attention())\n",
    "    \"\"\"\n",
    "    def __init__(self, step_dim,\n",
    "                 W_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "        self.supports_masking = True\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        self.step_dim = step_dim\n",
    "        self.features_dim = 0\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight((input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        self.features_dim = input_shape[-1]\n",
    "\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight((input_shape[1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "        else:\n",
    "            self.b = None\n",
    "        self.built = True\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        features_dim = self.features_dim\n",
    "        step_dim = self.step_dim\n",
    "        eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)),\n",
    "                        K.reshape(self.W, (features_dim, 1))), (-1, step_dim))\n",
    "        if self.bias:\n",
    "            eij += self.b\n",
    "        eij = K.tanh(eij)\n",
    "        a = K.exp(eij)\n",
    "        if mask is not None:\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "        a = K.expand_dims(a)\n",
    "        weighted_input = x * a\n",
    "        return K.sum(weighted_input, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0],  self.features_dim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b69d95ad878c49aa95143f6e309d56a49276312c"
   },
   "source": [
    "After defining the attention layer, we will define the entire model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "_uuid": "9c9b7132a66f89ad2cc2c3a6b9bbe6e639b56518"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 317)               0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 317, 200)          16638000  \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_1 (Spatial (None, 317, 200)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 317, 100)          100400    \n",
      "_________________________________________________________________\n",
      "attention_1 (Attention)      (None, 100)               417       \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 50)                5050      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 231)               11781     \n",
      "=================================================================\n",
      "Total params: 16,755,648\n",
      "Trainable params: 117,648\n",
      "Non-trainable params: 16,638,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def build_model(maxlen, vocab_size, embedding_size, embedding_matrix):\n",
    "    input_words = Input((maxlen, ))\n",
    "    x_words = Embedding(vocab_size,\n",
    "                        embedding_size,\n",
    "                        weights=[embedding_matrix],\n",
    "                        mask_zero=True,\n",
    "                        trainable=False)(input_words)\n",
    "    x_words = SpatialDropout1D(0.3)(x_words)\n",
    "    x_words = Bidirectional(LSTM(50, return_sequences=True))(x_words)\n",
    "    x = Attention(maxlen)(x_words)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = Dense(50, activation='relu')(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    pred = Dense(target_count, activation='softmax')(x)\n",
    "\n",
    "    model = Model(inputs=input_words, outputs=pred)\n",
    "    return model\n",
    "\n",
    "model = build_model(maxlen, vocab_size, embedding_size, embedding_matrix)\n",
    "model.compile(optimizer='nadam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ce6567202d90efe96afb0f0740e72d6b04b59526"
   },
   "source": [
    "# Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "_uuid": "18f7d4608277cc33958538f544edcb51a9b73549"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/15\n",
      "10000/10000 [==============================] - 11s 1ms/step - loss: 4.1450 - acc: 0.5279\n",
      "Epoch 2/15\n",
      "10000/10000 [==============================] - 9s 929us/step - loss: 0.4398 - acc: 0.7493\n",
      "Epoch 3/15\n",
      "10000/10000 [==============================] - 10s 1ms/step - loss: 0.1113 - acc: 0.7496\n",
      "Epoch 4/15\n",
      "10000/10000 [==============================] - 10s 981us/step - loss: 0.0857 - acc: 0.7495\n",
      "Epoch 5/15\n",
      "10000/10000 [==============================] - 10s 1ms/step - loss: 0.0755 - acc: 0.7497\n",
      "Epoch 6/15\n",
      "10000/10000 [==============================] - 9s 927us/step - loss: 0.0692 - acc: 0.7498\n",
      "Epoch 7/15\n",
      "10000/10000 [==============================] - 9s 928us/step - loss: 0.0659 - acc: 0.7496\n",
      "Epoch 8/15\n",
      "10000/10000 [==============================] - 9s 927us/step - loss: 0.0633 - acc: 0.7495\n",
      "Epoch 9/15\n",
      "10000/10000 [==============================] - 9s 928us/step - loss: 0.0596 - acc: 0.7498\n",
      "Epoch 10/15\n",
      "10000/10000 [==============================] - 9s 928us/step - loss: 0.0580 - acc: 0.7498\n",
      "Epoch 11/15\n",
      "10000/10000 [==============================] - 9s 939us/step - loss: 0.0558 - acc: 0.7499\n",
      "Epoch 12/15\n",
      "10000/10000 [==============================] - 10s 1ms/step - loss: 0.0548 - acc: 0.7499\n",
      "Epoch 13/15\n",
      "10000/10000 [==============================] - 9s 930us/step - loss: 0.0526 - acc: 0.7498\n",
      "Epoch 14/15\n",
      "10000/10000 [==============================] - 9s 929us/step - loss: 0.0506 - acc: 0.7499\n",
      "Epoch 15/15\n",
      "10000/10000 [==============================] - 9s 930us/step - loss: 0.0484 - acc: 0.7499\n"
     ]
    }
   ],
   "source": [
    "save_file = 'model.h5'\n",
    "history = model.fit(x_train, y_train,\n",
    "                    epochs=15, verbose=1,\n",
    "                    batch_size=1024, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "4cabc0bc4674270fa6f2d508f70fab16466ef646"
   },
   "source": [
    "# Making a submission file\n",
    "\n",
    "After training the model, we make a submission file by predicting for the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "_uuid": "e37e6c8afa82926ba70582f9241f711dfaee5e55"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000,)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = model.predict(x_test, batch_size=1024)\n",
    "y_pred = y_pred.argmax(axis=1).astype(int)\n",
    "y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "_uuid": "5230270f381badaaca5e9d47b623aa51d183a248"
   },
   "outputs": [],
   "source": [
    "df_test['prediction'] = y_pred\n",
    "df_test[['id', 'prediction']].to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
